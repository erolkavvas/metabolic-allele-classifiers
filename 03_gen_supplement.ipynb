{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# **03_gen_supplement.ipynb**: \n",
    "\n",
    "This ipython notebook interprets MAC results. Running this entire script generates a single file containing a summary of MAC results \n",
    "- `ENSEMBLE_DIR/supplementary/Supplementary File XYZ.xlsx`\n",
    "\n",
    "This code replaces previous results and is not additive like 01 and 02 .py scripts. Must be run after 01 and 02 py and results become interpretable with samples > 1000.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from cobra.io import load_json_model\n",
    "import sys,os,argparse,resource,warnings,itertools\n",
    "from os import listdir\n",
    "from mnc_utils import func_convert_gene2name_df, get_mnc_direction, correct_pvals, get_sig_feat_df, get_flux_df_anova, get_sprice_df_anova, get_pathway_enrichments\n",
    "from tqdm import tqdm\n",
    "from cobrascape.species import load_json_obj, save_json_obj\n",
    "import cobrascape.ensemble as ens\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Specify ENSEMBLE_DIR and pheno_list\n",
    "\n",
    "gene_to_pathways is recommended for pathway analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t... creating directory for supplementary data:mnc_ensemble_0/supplementary/\n"
     ]
    }
   ],
   "source": [
    "ENSEMBLE_DIR = \"mnc_ensemble_0\"\n",
    "pheno_list = [\"pyrazinamide\"]\n",
    "gene_to_pathways = load_json_obj(\"cobra_model/gene_to_pathways_filt.json\")\n",
    "\n",
    "## TESTSET is set to False automatically since the interpretations are available for the training set, not test set.\n",
    "## The test set is used in validating the MACs\n",
    "TESTSET = False \n",
    "\n",
    "## SPRICE_RCOST_BOOL determines whether the median shadow price and reduced cost is calculated per strain. This\n",
    "## step is very time consuming so the default is False. Only median fluxes are determined.\n",
    "SPRICE_RCOST_BOOL = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set other parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action_num (4), nabound (False), standardize (False)\n",
      "input: (S)toichimetric genome-scale model= (genes: 905, reactions: 1071, metabolites: 855)\n"
     ]
    }
   ],
   "source": [
    "bic_thresh_val = 10\n",
    "save_data=True\n",
    "med_or_mean='median'#\"mean\"#'median'\n",
    "obj_direct_norm = True\n",
    "\n",
    "ENSEMBLE_MAP_ASSESS = ENSEMBLE_DIR+\"/popfva_assessment/\"\n",
    "SUPP_FILE_LOC = ENSEMBLE_DIR+\"/supplementary/\"\n",
    "if not os.path.exists(SUPP_FILE_LOC):\n",
    "    print('\\t... creating directory for supplementary data:'+SUPP_FILE_LOC)\n",
    "    os.makedirs(SUPP_FILE_LOC)\n",
    "\n",
    "ensemble_args_dict = load_json_obj(ENSEMBLE_DIR+\"/mnc_ensemble_args.json\")\n",
    "action_num = ensemble_args_dict[\"action_num\"] # 4 \n",
    "ADD_NA_BOUND = ensemble_args_dict[\"nabound\"] # False\n",
    "STANDARDIZE_ = ensemble_args_dict[\"popFVA_STANDARDIZE\"] # False\n",
    "print(\"action_num (%d), nabound (%s), standardize (%s)\"%(action_num, str(ADD_NA_BOUND),str(STANDARDIZE_)))\n",
    "\n",
    "COBRA_MODEL = load_json_model(ENSEMBLE_DIR+\"/base_cobra_model.json\")\n",
    "print(\"input: (S)toichimetric genome-scale model= (genes: %d, reactions: %d, metabolites: %d)\" % (len(COBRA_MODEL.genes), \n",
    "    len(COBRA_MODEL.reactions), len(COBRA_MODEL.metabolites)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyze MACs and generate results tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 3236.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...loading SAMPLES_ASSESS_DF to identify minimum BIC or AIC MNCs\n",
      "\t... SAMPLES_ASSESS_DF shape: (samples: 2, assess_cols: 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "##########################################\n",
    "# 1. BIC of MACs\n",
    "##########################################\n",
    "\n",
    "def load_samples_assess_df(ENSEMBLE_MAP_ASSESS, pheno_list):\n",
    "    ### -------------- LOAD 2 -----------------\n",
    "    print(\"...loading SAMPLES_ASSESS_DF to identify minimum BIC or AIC MNCs\")\n",
    "    onlyfiles = [f for f in listdir(ENSEMBLE_MAP_ASSESS) if os.path.isfile(os.path.join(ENSEMBLE_MAP_ASSESS, f))]\n",
    "    onlyfiles = [f for f in onlyfiles if f != \".DS_Store\"]\n",
    "    samplesAfter = [f for f in onlyfiles if \"sample_\" in f] \n",
    "\n",
    "    wanted_keys = []\n",
    "    ### Options for what we want in SAMPLES_ASSESS_DF\n",
    "    for pheno_id in pheno_list:\n",
    "        wanted_keys.extend([\"AIC_\"+pheno_id, \"BIC_\"+pheno_id])\n",
    "\n",
    "    SAMPLES_ASSESS_DF = {}\n",
    "    for landscape_sample_name in tqdm(samplesAfter):\n",
    "        landscape_sample_num = landscape_sample_name.split(\"_\")[1]\n",
    "        sample_id = \"sampled_map_\"+str(landscape_sample_num)\n",
    "        landscape_assess_sample_file = ENSEMBLE_MAP_ASSESS+landscape_sample_name\n",
    "\n",
    "        if os.path.exists(landscape_assess_sample_file):\n",
    "            landscape_assess_sample = load_json_obj(landscape_assess_sample_file)\n",
    "            SAMPLES_ASSESS_DF[sample_id] = {}\n",
    "            SAMPLES_ASSESS_DF[sample_id].update(dict((k, landscape_assess_sample[k]) for k in wanted_keys if k in landscape_assess_sample))\n",
    "\n",
    "    # transform to pandas dataframe\n",
    "    SAMPLES_ASSESS_DF = pd.DataFrame.from_dict(SAMPLES_ASSESS_DF,orient=\"index\")\n",
    "    print(\"\\t... SAMPLES_ASSESS_DF shape: (samples: %d, assess_cols: %d)\" % (SAMPLES_ASSESS_DF.shape[0], SAMPLES_ASSESS_DF.shape[1]))\n",
    "    return SAMPLES_ASSESS_DF\n",
    "\n",
    "\n",
    "SAMPLES_ASSESS_DF = load_samples_assess_df(ENSEMBLE_MAP_ASSESS, pheno_list)\n",
    "\n",
    "for pheno_id in pheno_list:\n",
    "    top_models = SAMPLES_ASSESS_DF[\"BIC_\"+pheno_id].copy()\n",
    "    top_models.sort_values(inplace=True)\n",
    "    top_models.to_csv(ENSEMBLE_DIR+\"/tables/best_mncs_\"+pheno_id+\".csv\",header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 1305.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of pyrazinamide MACs: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "##########################################\n",
    "# 2. Objective functions of MACs\n",
    "##########################################\n",
    "for pheno_id in pheno_list:\n",
    "    \n",
    "    ### Get sample ids for best antibiotic-specific MNCs\n",
    "    top_models = pd.read_csv(ENSEMBLE_DIR+\"/tables/best_mncs_\"+pheno_id+\".csv\",index_col=0)\n",
    "    top_models = top_models[top_models - top_models.min() < bic_thresh_val].dropna()\n",
    "    sample_list = top_models.index.tolist()\n",
    "    print(\"Number of %s MACs: %d\"%(pheno_id, len(sample_list)))\n",
    "    \n",
    "    obj_allsamples_df = pd.DataFrame()\n",
    "    obj_direction_minmax = \"max\"\n",
    "    r_filt_allsamples_dict = {}\n",
    "    for sample_id in tqdm(sample_list):\n",
    "        obj_sample_fn = ENSEMBLE_DIR+\"/mnc_objectives/\"+\"obj_\"+sample_id+\"__\"+pheno_id+\".json\"\n",
    "        r_obj_sample_dict = load_json_obj(obj_sample_fn)\n",
    "        r_filt_allsamples_dict.update(r_obj_sample_dict)\n",
    "\n",
    "    obj_allsamples_df = pd.DataFrame.from_dict(r_filt_allsamples_dict,orient=\"index\")\n",
    "\n",
    "    ### Align the objectives in high quality models by have them all be maximizing. \n",
    "    obj_hqsamples_max_df = obj_allsamples_df.copy() # .fillna(0)\n",
    "    for sampled_map_id, coef_row in obj_allsamples_df.iterrows():\n",
    "        sample_id = \"sample_\"+sampled_map_id.split(\"_\")[-1]\n",
    "        obj_direct = get_mnc_direction(pheno_id, sample_id, ENSEMBLE_DIR)\n",
    "\n",
    "        ### If the direction is a minimization, multiple all objective coefficients by -1.\n",
    "        if obj_direct[0] == \"min\":\n",
    "            obj_hqsamples_max_df.loc[sampled_map_id] = -1*obj_hqsamples_max_df.loc[sampled_map_id]\n",
    "\n",
    "    obj_hqsamples_abs_df = abs(obj_hqsamples_max_df)\n",
    "    obj_hqsamples_max_df\n",
    "\n",
    "    obj_avg_df = obj_hqsamples_max_df.apply(lambda x: x.mean())\n",
    "    obj_med_df = obj_hqsamples_max_df.apply(lambda x: x.median())\n",
    "\n",
    "    obj_avg_abs_df = obj_hqsamples_abs_df.apply(lambda x: x.mean())\n",
    "    obj_med_abs_df = obj_hqsamples_abs_df.apply(lambda x: x.median())\n",
    "    \n",
    "    if save_data==True:\n",
    "        obj_hqsamples_max_df.to_csv(ENSEMBLE_DIR+\"/tables/mnc_objectives_\"+pheno_id+\"__MAX\"+\".csv\",header=True)\n",
    "        obj_hqsamples_abs_df.to_csv(ENSEMBLE_DIR+\"/tables/mnc_objectives_\"+pheno_id+\"__MAX-ABS\"+\".csv\",header=True)\n",
    "        obj_avg_df.to_csv(ENSEMBLE_DIR+\"/tables/mnc_objectives_\"+pheno_id+\"__avg\"+\".csv\",header=True)\n",
    "        obj_med_df.to_csv(ENSEMBLE_DIR+\"/tables/mnc_objectives_\"+pheno_id+\"__med\"+\".csv\",header=True)\n",
    "        obj_avg_abs_df.to_csv(ENSEMBLE_DIR+\"/tables/mnc_objectives_\"+pheno_id+\"__avg-abs\"+\".csv\",header=True)\n",
    "        obj_med_abs_df.to_csv(ENSEMBLE_DIR+\"/tables/mnc_objectives_\"+pheno_id+\"__med-abs\"+\".csv\",header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...loading TRAINING data...\n",
      "input: (G)enetic variant matrix= (strains: 159, alleles: 12762)\n"
     ]
    }
   ],
   "source": [
    "if TESTSET==False:\n",
    "    print(\"...loading TRAINING data...\")\n",
    "    X_species_final = pd.read_csv(ENSEMBLE_DIR+\"/X_train.csv\", index_col = 0)\n",
    "    Y_pheno_final = pd.read_csv(ENSEMBLE_DIR+\"/y_train.csv\", index_col = 0)\n",
    "    file_outtag = \"train\"\n",
    "elif TESTSET==True:\n",
    "    print(\"...loading TEST data...\")\n",
    "    X_species_final = pd.read_csv(ENSEMBLE_DIR+\"/X_test.csv\", index_col = 0)\n",
    "    Y_pheno_final = pd.read_csv(ENSEMBLE_DIR+\"/y_test.csv\", index_col = 0)\n",
    "    file_outtag = \"test\"\n",
    "\n",
    "print(\"input: (G)enetic variant matrix= (strains: %d, alleles: %d)\" % (X_species_final.shape[0], X_species_final.shape[1]))\n",
    "\n",
    "### Load in the genetic variant matrix and AMR phenotypes for each case.\n",
    "pheno_to_data2d_dict = {}\n",
    "pheno_to_Y_dict = {}\n",
    "if TESTSET==False:\n",
    "    ALLELE_PHENO_FILE = ENSEMBLE_DIR+\"/allele_pheno_data/\"\n",
    "    for pheno_id in pheno_list:\n",
    "        G_VARIANT_MATRIX_FILE = ALLELE_PHENO_FILE+\"/allele_df_\"+pheno_id+\".csv\"\n",
    "        PHENO_MATRIX_FILE = ALLELE_PHENO_FILE+\"/pheno_df_\"+pheno_id+\".csv\"\n",
    "        pheno_to_data2d_dict.update({pheno_id: pd.read_csv(G_VARIANT_MATRIX_FILE,index_col=0)})\n",
    "        pheno_to_Y_dict.update({pheno_id: pd.read_csv(PHENO_MATRIX_FILE,index_col=0)[pheno_id]}) ### series\n",
    "elif TESTSET==True:\n",
    "    for pheno_id in pheno_list:\n",
    "        X_filtered, Y_filtered = cs.filter_pheno_nan(X_species_final, Y_pheno_final, pheno_id)\n",
    "        pheno_to_data2d_dict.update({pheno_id: X_filtered.loc[:,allele_list]})\n",
    "        pheno_to_Y_dict.update({pheno_id: Y_filtered}) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of pyrazinamide MACs: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  3.55it/s]\n",
      "/Users/erolkavvas/Documents/metabolic-allele-classifiers/mnc_utils.py:198 \u001b[1;31mRuntimeWarning\u001b[0m: divide by zero encountered in log10\n",
      "  0%|          | 0/159 [00:00<?, ?it/s]/Users/erolkavvas/anaconda3/lib/python3.7/site-packages/numpy/lib/nanfunctions.py:1076 \u001b[1;31mRuntimeWarning\u001b[0m: Mean of empty slice\n",
      "100%|██████████| 159/159 [01:55<00:00,  1.37it/s]\n",
      "/Users/erolkavvas/anaconda3/lib/python3.7/site-packages/sklearn/feature_selection/univariate_selection.py:114 \u001b[1;31mUserWarning\u001b[0m: Features [   1    5    8   11   13   19   20   21   22   25   27   33   34   35\n",
      "   42   54   57   59   60   68   69   70   71   76   79   85   88   90\n",
      "   91   94   95   97   99  100  101  103  105  123  125  126  127  128\n",
      "  133  134  135  137  138  139  142  155  156  180  183  185  190  191\n",
      "  194  195  198  201  203  206  207  208  209  211  212  213  217  219\n",
      "  220  221  222  224  225  226  227  228  229  230  233  234  235  236\n",
      "  239  242  245  246  247  248  249  252  253  254  255  256  257  259\n",
      "  262  263  264  265  266  268  269  284  285  287  296  322  324  325\n",
      "  326  328  333  336  338  341  354  357  361  362  364  378  379  383\n",
      "  386  398  401  403  404  409  410  411  412  420  421  422  425  429\n",
      "  442  451  456  459  463  464  465  468  469  471  472  476  478  479\n",
      "  487  488  495  499  502  503  508  509  538  539  540  546  563  565\n",
      "  566  568  571  573  574  575  576  577  578  579  580  581  583  584\n",
      "  588  590  593  594  595  599  600  601  605  616  619  620  621  622\n",
      "  645  654  656  662  683  684  692  707  719  720  728  729  733  734\n",
      "  737  738  745  746  747  748  752  756  757  767  774  784  788  790\n",
      "  791  793  794  795  811  812  828  829  831  834  835  836  837  838\n",
      "  840  842  843  844  845  846  847  849  852  881  882  884  887  888\n",
      "  889  928  929  930  946  948  950  951  952  953  954  955  961  974\n",
      "  977  980  987  988 1000 1002 1008 1009 1010 1011 1012 1015 1016 1017\n",
      " 1018 1023 1025 1026 1033 1034 1035 1037 1038 1039 1040 1041 1043 1044\n",
      " 1045 1047 1049 1052 1053 1054 1055 1061 1063 1064 1065 1066 1067 1069] are constant.\n",
      "/Users/erolkavvas/anaconda3/lib/python3.7/site-packages/sklearn/feature_selection/univariate_selection.py:115 \u001b[1;31mRuntimeWarning\u001b[0m: divide by zero encountered in true_divide\n",
      "/Users/erolkavvas/anaconda3/lib/python3.7/site-packages/sklearn/feature_selection/univariate_selection.py:115 \u001b[1;31mRuntimeWarning\u001b[0m: invalid value encountered in true_divide\n",
      "/Users/erolkavvas/anaconda3/lib/python3.7/site-packages/sklearn/feature_selection/univariate_selection.py:114 \u001b[1;31mUserWarning\u001b[0m: Features [   1    5    8   11   13   19   20   21   22   25   27   33   34   35\n",
      "   42   54   57   59   60   68   69   70   71   76   79   85   88   90\n",
      "   91   94   95   97   99  100  101  103  105  123  125  126  127  128\n",
      "  133  134  135  137  138  139  142  155  156  180  183  185  190  191\n",
      "  194  195  198  201  203  206  207  208  209  211  212  213  217  219\n",
      "  220  221  222  224  225  226  227  228  229  230  233  234  235  236\n",
      "  239  242  245  246  247  248  249  252  253  254  255  256  257  259\n",
      "  262  263  264  265  266  268  269  284  285  287  296  322  324  325\n",
      "  326  328  333  336  338  341  354  357  361  362  364  378  379  383\n",
      "  386  398  401  403  404  409  410  411  412  420  421  422  425  429\n",
      "  442  451  456  459  463  464  465  468  469  471  472  476  478  479\n",
      "  487  488  495  499  502  503  508  509  538  539  540  546  563  565\n",
      "  566  568  571  573  574  575  576  577  578  579  580  581  583  584\n",
      "  588  590  593  594  595  599  600  601  605  616  619  620  621  622\n",
      "  645  654  656  662  683  684  692  707  719  720  728  729  733  734\n",
      "  737  738  745  746  747  748  752  756  757  767  774  784  788  790\n",
      "  791  793  794  795  811  812  828  829  831  834  835  836  837  838\n",
      "  840  842  843  844  845  846  847  849  852  881  882  884  887  888\n",
      "  889  928  929  930  946  948  950  951  952  953  954  955  961  974\n",
      "  977  980  987  988 1000 1002 1008 1009 1010 1011 1012 1015 1016 1017\n",
      " 1018 1023 1025 1026 1033 1034 1035 1037 1038 1039 1040 1041 1043 1044\n",
      " 1045 1047 1049 1052 1053 1054 1055 1061 1063 1064 1065 1066 1067 1069] are constant.\n",
      "/Users/erolkavvas/anaconda3/lib/python3.7/site-packages/sklearn/feature_selection/univariate_selection.py:115 \u001b[1;31mRuntimeWarning\u001b[0m: divide by zero encountered in true_divide\n",
      "/Users/erolkavvas/anaconda3/lib/python3.7/site-packages/sklearn/feature_selection/univariate_selection.py:115 \u001b[1;31mRuntimeWarning\u001b[0m: invalid value encountered in true_divide\n"
     ]
    }
   ],
   "source": [
    "##########################################\n",
    "# 3. Flux states of MACs (may take a while...)\n",
    "##########################################\n",
    "for pheno_id in pheno_list:\n",
    "    top_models = pd.read_csv(ENSEMBLE_DIR+\"/tables/best_mncs_\"+pheno_id+\".csv\",index_col=0)\n",
    "    top_models = top_models[top_models - top_models.min() < bic_thresh_val].dropna()\n",
    "    sample_list = top_models.index.tolist()\n",
    "    print(\"Number of %s MACs: %d\"%(pheno_id, len(sample_list)))\n",
    "    sig_flux_df, sol_dict = get_sig_feat_df(ENSEMBLE_DIR, pheno_id, sample_list, pheno_to_Y_dict,\n",
    "                                            feat_type=\"pop_fluxes\",med_or_mean=med_or_mean) # 'mean'\n",
    "    sample_list = [\"sample_\"+x.split(\"_\")[-1] for x in sample_list]\n",
    "    popfva_longform_all_df = pd.DataFrame()\n",
    "    flux_sol_all_df, sprice_sol_all_df, rcosts_sol_all_df = pd.DataFrame(), pd.DataFrame(), pd.DataFrame()\n",
    "    \n",
    "    for sample_id in sample_list[:]:\n",
    "        obj_direction, _ = get_mnc_direction(pheno_id, sample_id, ENSEMBLE_DIR)\n",
    "        \n",
    "        flux_sol_df = sol_dict[sample_id][pheno_id][\"sol\"][\"pop_fluxes\"].copy()\n",
    "        sprice_sol_df = sol_dict[sample_id][pheno_id][\"sol\"][\"pop_sprices\"].copy()\n",
    "        rcosts_sol_df = sol_dict[sample_id][pheno_id][\"sol\"][\"pop_rcosts\"].copy()\n",
    "        if obj_direction==\"min\" and obj_direct_norm==True:\n",
    "            flux_sol_df = 1 - flux_sol_df\n",
    "\n",
    "        sampled_map_num = \"sampled_map_\"+str(sample_id.split(\"_\")[1])\n",
    "        flux_sol_df[\"index\"] = flux_sol_df.index\n",
    "        # flux_sol_df[\"sample_id\"] = sampled_map_num\n",
    "        flux_sol_df[pheno_id] = pheno_to_Y_dict[pheno_id]\n",
    "        flux_sol_all_df = pd.concat([flux_sol_all_df, flux_sol_df],axis=0,ignore_index=True)\n",
    "\n",
    "        sprice_sol_df[\"index\"] = sprice_sol_df.index\n",
    "        sprice_sol_df[pheno_id] = pheno_to_Y_dict[pheno_id]\n",
    "        # sprice_sol_df[\"sample_id\"] = sampled_map_num\n",
    "        sprice_sol_all_df = pd.concat([sprice_sol_all_df, sprice_sol_df],axis=0,ignore_index=True)\n",
    "\n",
    "        rcosts_sol_df[\"index\"] = rcosts_sol_df.index\n",
    "        rcosts_sol_df[pheno_id] = pheno_to_Y_dict[pheno_id]\n",
    "        # rcosts_sol_df[\"sample_id\"] = sampled_map_num\n",
    "        rcosts_sol_all_df = pd.concat([rcosts_sol_all_df, rcosts_sol_df],axis=0,ignore_index=True)\n",
    "\n",
    "\n",
    "    ### Average the fluxes per strain and perform MNC flux GWAS\n",
    "    strain_flux_avg_df = flux_sol_df.copy()\n",
    "    strain_flux_med_df = flux_sol_df.copy()\n",
    "    for strain in tqdm(flux_sol_df.index.unique()[:]):\n",
    "        strain_flux_avg_df.loc[strain]=flux_sol_all_df[(flux_sol_all_df[\"index\"]==strain)].apply(lambda x: x.mean(), axis=0)\n",
    "        strain_flux_med_df.loc[strain]=flux_sol_all_df[(flux_sol_all_df[\"index\"]==strain)].apply(lambda x: x.median(), axis=0)\n",
    "\n",
    "    strain_flux_avg_df[\"index\"] = strain_flux_avg_df.index\n",
    "    strain_flux_avg_df[pheno_id] = pheno_to_Y_dict[pheno_id]\n",
    "\n",
    "    strain_flux_med_df[\"index\"] = strain_flux_med_df.index\n",
    "    strain_flux_med_df[pheno_id] = pheno_to_Y_dict[pheno_id]\n",
    "\n",
    "    strain_flux_avg_df.to_csv(ENSEMBLE_DIR+\"/tables/strain_flux_avg_df_\"+pheno_id+\"__objnorm-\"+str(obj_direct_norm)+\".csv\")\n",
    "    strain_flux_med_df.to_csv(ENSEMBLE_DIR+\"/tables/strain_flux_med_df_\"+pheno_id+\"__objnorm-\"+str(obj_direct_norm)+\".csv\")\n",
    "\n",
    "    popdf_mean_flux_anova_df = get_flux_df_anova(strain_flux_avg_df.dropna(), pheno_id, sol_dict[sample_id][pheno_id][\"anova\"][\"pop_fluxes\"])\n",
    "    popdf_median_flux_anova_df = get_flux_df_anova(strain_flux_med_df.dropna(), pheno_id, sol_dict[sample_id][pheno_id][\"anova\"][\"pop_fluxes\"])\n",
    "\n",
    "    popdf_mean_flux_anova_df.to_csv(ENSEMBLE_DIR+\"/tables/strain_flux_avg_ANOVA_df_\"+pheno_id+\"__objnorm-\"+str(obj_direct_norm)+\".csv\")\n",
    "    popdf_median_flux_anova_df.to_csv(ENSEMBLE_DIR+\"/tables/strain_flux_med_ANOVA_df_\"+pheno_id+\"__objnorm-\"+str(obj_direct_norm)+\".csv\")\n",
    "    \n",
    "    ### Average the metabolite shadow prices per strain and perform MNC flux GWAS\n",
    "    if SPRICE_RCOST_BOOL==True:\n",
    "        strain_sprice_avg_df = sprice_sol_df.copy()\n",
    "        strain_sprice_med_df = sprice_sol_df.copy()\n",
    "        for strain in tqdm(sprice_sol_df.index.unique()[:]):\n",
    "            strain_sprice_avg_df.loc[strain]=sprice_sol_all_df[(sprice_sol_all_df[\"index\"]==strain)].apply(lambda x: x.mean(), axis=0)\n",
    "            strain_sprice_med_df.loc[strain]=sprice_sol_all_df[(sprice_sol_all_df[\"index\"]==strain)].apply(lambda x: x.median(), axis=0)\n",
    "\n",
    "        strain_sprice_avg_df[\"index\"] = strain_sprice_avg_df.index\n",
    "        strain_sprice_avg_df[pheno_id] = pheno_to_Y_dict[pheno_id]\n",
    "\n",
    "        strain_sprice_med_df[\"index\"] = strain_sprice_med_df.index\n",
    "        strain_sprice_med_df[pheno_id] = pheno_to_Y_dict[pheno_id]\n",
    "\n",
    "        strain_sprice_avg_df.to_csv(ENSEMBLE_DIR+\"/tables/strain_sprice_avg_df_\"+pheno_id+\"__objnorm-\"+str(obj_direct_norm)+\".csv\")\n",
    "        strain_sprice_med_df.to_csv(ENSEMBLE_DIR+\"/tables/strain_sprice_med_df_\"+pheno_id+\"__objnorm-\"+str(obj_direct_norm)+\".csv\")\n",
    "\n",
    "        popdf_mean_sprice_anova_df = get_sprice_df_anova(strain_sprice_avg_df.dropna(), pheno_id, sol_dict[sample_id][pheno_id][\"anova\"][\"pop_sprices\"])\n",
    "        popdf_median_sprice_anova_df = get_sprice_df_anova(strain_sprice_med_df.dropna(), pheno_id, sol_dict[sample_id][pheno_id][\"anova\"][\"pop_sprices\"])\n",
    "\n",
    "        popdf_mean_sprice_anova_df.to_csv(ENSEMBLE_DIR+\"/tables/strain_sprice_avg_ANOVA_df_\"+pheno_id+\"__objnorm-\"+str(obj_direct_norm)+\".csv\")\n",
    "        popdf_median_sprice_anova_df.to_csv(ENSEMBLE_DIR+\"/tables/strain_sprice_med_ANOVA_df_\"+pheno_id+\"__objnorm-\"+str(obj_direct_norm)+\".csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/erolkavvas/anaconda3/lib/python3.7/site-packages/statsmodels/stats/multitest.py:147 \u001b[1;31mRuntimeWarning\u001b[0m: invalid value encountered in less_equal\n",
      "/Users/erolkavvas/anaconda3/lib/python3.7/site-packages/statsmodels/stats/multitest.py:251 \u001b[1;31mRuntimeWarning\u001b[0m: invalid value encountered in greater\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 9)\n",
      "1e-20 (2, 8)\n",
      "\t 0\n",
      "\t 2\n",
      "\t 2\n",
      "\t 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/erolkavvas/Documents/metabolic-allele-classifiers/mnc_utils.py:509 \u001b[1;31mSettingWithCopyWarning\u001b[0m: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "##########################################\n",
    "# 4. Pathway enrichments of MACs\n",
    "##########################################\n",
    "if type(gene_to_pathways)==dict:\n",
    "    all_subsys_phenos = pd.DataFrame()\n",
    "    SUBSYS_NUM_THRESHOLD = 2\n",
    "    add_small_pval = 1e-20\n",
    "    save_fig = False\n",
    "    kegg_or_biocyc = \"biocyc\"#\"kegg\" #\"biocyc\"#\"kegg\"\n",
    "    if kegg_or_biocyc==\"kegg\":\n",
    "        g_to_pathways = kegg_gene_to_paths\n",
    "    elif kegg_or_biocyc==\"biocyc\":\n",
    "        g_to_pathways = gene_to_pathways\n",
    "\n",
    "    ###  Loop through each drug and output flux data and pathway enrichments\n",
    "    for pheno_id in pheno_list: \n",
    "        strain_flux_med_df=pd.read_csv(ENSEMBLE_DIR+\"/tables/strain_flux_med_df_\"+pheno_id+\"__objnorm-\"+str(obj_direct_norm)+\".csv\",index_col=0)\n",
    "        strain_flux_avg_df=pd.read_csv(ENSEMBLE_DIR+\"/tables/strain_flux_avg_df_\"+pheno_id+\"__objnorm-\"+str(obj_direct_norm)+\".csv\",index_col=0)\n",
    "        popdf_mean_flux_anova_df = pd.read_csv(ENSEMBLE_DIR+\"/tables/strain_flux_avg_ANOVA_df_\"+pheno_id+\"__objnorm-\"+str(obj_direct_norm)+\".csv\",index_col=0)\n",
    "        popdf_median_flux_anova_df = pd.read_csv(ENSEMBLE_DIR+\"/tables/strain_flux_med_ANOVA_df_\"+pheno_id+\"__objnorm-\"+str(obj_direct_norm)+\".csv\",index_col=0)\n",
    "\n",
    "        popdf_median_flux_anova_df_correct = correct_pvals(popdf_median_flux_anova_df,pval_col=\"pvalue\", \n",
    "                                                       method=\"bonferroni\", correct_alpha=0.05) # \"fdr_bh\"\n",
    "        print(popdf_median_flux_anova_df_correct.shape)\n",
    "        if popdf_median_flux_anova_df_correct.shape[0]!=0:\n",
    "            \n",
    "            subsys_sig_median, react_to_subsys, subsys_to_sig_react = get_pathway_enrichments(ENSEMBLE_DIR,\n",
    "                popdf_median_flux_anova_df, pheno_id, g_to_pathways,kegg_biocyc=kegg_or_biocyc, med_or_med=\"median\",\n",
    "                pval_thresh=popdf_median_flux_anova_df_correct[\"pvalue\"].max()+add_small_pval,\n",
    "                save_data=True\n",
    "            )\n",
    "            subsys_sig_median_filt = subsys_sig_median[subsys_sig_median[\"TOTAL_SUBSYS_NUM\"]>SUBSYS_NUM_THRESHOLD]\n",
    "            # all_subsys_phenos = pd.concat([all_subsys_phenos, subsys_sig_median_filt[[\"subsys_pval_\"+pheno_id]]],axis=1)\n",
    "\n",
    "            subsys_sig_median_FDR = correct_pvals(subsys_sig_median_filt,pval_col=\"subsys_pval_\"+pheno_id, method=\"fdr_bh\", correct_alpha=0.05)\n",
    "            # subsys_sig_median_FDR = correct_pvals(subsys_sig_median_filt,pval_col=\"subsys_pval_\"+pheno_id, method=\"fdr_bh\", correct_alpha=0.05)\n",
    "            subsys_sig_median_FDR.to_csv(ENSEMBLE_DIR+\"/tables/pathway_enriched_FDR_\"+pheno_id+\".csv\")\n",
    "        else:\n",
    "            print(\"No significant fluxes according to ANOVA F-test. You need to generate more MAC samples!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################################\n",
    "# 4. Pathway enrichments of MACs\n",
    "##########################################\n",
    "def get_constraint_df(sample_list, pheno_to_data2d_dict, ENSEMBLE_MAP_ASSESS,gene_to_name=None,gene_name=True):\n",
    "    \"\"\" Takes in a list of MNC samples and returns their allele-constraint maps (alleles vs samples dataframe)\n",
    "        The values are in categorical terms, i.e. \"lb_0\", \"ub_2\", etc...\n",
    "    \"\"\"\n",
    "    pheno_id = list(pheno_to_data2d_dict.keys())[0] # arbitrary, just need to get one key in dict\n",
    "    allele_col_ids = [x for x in pheno_to_data2d_dict[pheno_id].columns]\n",
    "\n",
    "    SAMPLES_AC_DF = {}\n",
    "    for sample_id in sample_list:\n",
    "        landscape_sample_num = sample_id.split(\"_\")[-1]\n",
    "        landscape_sample_name = \"sample_\"+str(landscape_sample_num)+\"_map_assess.json\"\n",
    "        landscape_assess_sample_file = ENSEMBLE_MAP_ASSESS+landscape_sample_name\n",
    "\n",
    "        if os.path.exists(landscape_assess_sample_file):\n",
    "            landscape_assess_sample = load_json_obj(landscape_assess_sample_file)\n",
    "            SAMPLES_AC_DF[sample_id] = {}\n",
    "            SAMPLES_AC_DF[sample_id].update(dict((k, landscape_assess_sample[k]) for k in allele_col_ids if k in landscape_assess_sample))\n",
    "\n",
    "    SAMPLES_AC_DF = pd.DataFrame.from_dict(SAMPLES_AC_DF,orient=\"index\")\n",
    "    # print(\"\\t... SAMPLES_AC_DF shape: (samples: %d, assess_cols: %d)\" % (SAMPLES_AC_DF.shape[0], SAMPLES_AC_DF.shape[1]))\n",
    "    if gene_name==True and gene_to_name!=None:\n",
    "        SAMPLES_AC_DF = func_convert_gene2name_df(SAMPLES_AC_DF.T, gene_to_name)\n",
    "        SAMPLES_AC_DF = SAMPLES_AC_DF[sample_list]\n",
    "    else:\n",
    "        SAMPLES_AC_DF = SAMPLES_AC_DF.T\n",
    "        SAMPLES_AC_DF = SAMPLES_AC_DF[sample_list]\n",
    "    return SAMPLES_AC_DF\n",
    "\n",
    "\n",
    "def get_allele_lor_dict(allele_list, pheno_id, x_allele_dict, y_pheno_dict, gene_to_name=None,gene_name=True):\n",
    "    \"\"\" Takes in a list of alleles and a phenotype and returns a dictionary describing its log odds ratio,\n",
    "        number of strains with allele, and number of strains with the resistant phenotype.\n",
    "    \"\"\"\n",
    "    allele_df = pheno_to_data2d_dict[pheno_id]\n",
    "    if gene_name==True:\n",
    "        allele_df = func_convert_gene2name_df(allele_df.T, gene_to_name)\n",
    "        allele_df = allele_df.T\n",
    "        \n",
    "    lor_dict = {}\n",
    "    for allele in allele_list:\n",
    "        num_strains_with_allele = len(allele_df[allele_df[allele]==1].index.tolist())\n",
    "        LOR, num_R = ens.log_odds_ratio(allele, allele_df, y_pheno_dict[pheno_id], addval=0.5)\n",
    "        lor_dict.update({allele: {\"lor\": LOR, \"num_strains\":num_strains_with_allele, \"num_resist\": num_R}})\n",
    "    return lor_dict\n",
    "\n",
    "\n",
    "def align_constraints(sample_list, samples_ac_df, ENSEMBLE_DIR, pheno_id, COBRA_MODEL, coef_thresh=0.0, verbose=False):\n",
    "    \"\"\" Aligns the constraints according to the maximization direction.\n",
    "        Requires \".../tables/mnc_objectives_\"+pheno_id+\"__MAX\"+\".csv\"\n",
    "    \n",
    "    \"\"\"\n",
    "    sample_id_list = [\"sample_\"+x.split(\"_\")[-1] for x in sample_list]\n",
    "    samples_ac_df_align = samples_ac_df.copy()\n",
    "    obj_hqsamples_max_df=pd.read_csv(ENSEMBLE_DIR+\"/tables/mnc_objectives_\"+pheno_id+\"__MAX\"+\".csv\",index_col=0)\n",
    "    # obj_hqsamples_abs_df=pd.read_csv(ENSEMBLE_DIR+\"/tables/mnc_objectives_\"+pheno_id+\"__MAX-ABS\"+\".csv\",index_col=0)\n",
    "    # obj_avg_df.to_csv(ENSEMBLE_DIR+\"/tables/mnc_objectives_\"+pheno_id+\"__avg\"+\".csv\",header=True)\n",
    "    # obj_med_df.to_csv(ENSEMBLE_DIR+\"/tables/mnc_objectives_\"+pheno_id+\"__med\"+\".csv\",header=True)\n",
    "    # obj_avg_abs_df.to_csv(ENSEMBLE_DIR+\"/tables/mnc_objectives_\"+pheno_id+\"__avg-abs\"+\".csv\",header=True)\n",
    "    # obj_med_abs_df=pd.read_csv(ENSEMBLE_DIR+\"/tables/mnc_objectives_\"+pheno_id+\"__med-abs\"+\".csv\",index_col=0,header=None)[1]\n",
    "    \n",
    "    for sampled_map_num in tqdm(sample_list[:]):\n",
    "\n",
    "        sample_id = \"sample_\"+sampled_map_num.split(\"_\")[-1]\n",
    "        obj_direction, pvals = get_mnc_direction(pheno_id, sample_id, ENSEMBLE_DIR)\n",
    "        if verbose==True:\n",
    "            print(sample_id, obj_direction, pvals)\n",
    "        alleles_changed = []\n",
    "        \n",
    "        # sort dataframe based on absolute values of hqsamples max\n",
    "        rxnvar_order = abs(obj_hqsamples_max_df.loc[sampled_map_num][:]).sort_values(ascending=False).index.tolist()\n",
    "        for obj_var, coef in obj_hqsamples_max_df.loc[sampled_map_num][rxnvar_order].items():\n",
    "        \n",
    "            rxn = \"_\".join(obj_var.split(\"_\")[:-1])\n",
    "            rxn_genes = [x.id for x in COBRA_MODEL.reactions.get_by_id(rxn).genes]\n",
    "            rxn_alleles = [x for x in allele_list if x.split(\"_\")[0] in rxn_genes]\n",
    "            ## Don't change those that have already been changed...\n",
    "            rxn_alleles = [x for x in rxn_alleles if x not in alleles_changed]\n",
    "            if verbose==True:\n",
    "                print(\"\\t\",obj_var,coef) # , rxn_alleles\n",
    "                print(\"\\t\",samples_ac_df.loc[rxn_alleles][sampled_map_num].to_dict())\n",
    "\n",
    "            lb_alleles = samples_ac_df_align.loc[rxn_alleles][sampled_map_num][samples_ac_df_align.loc[rxn_alleles][sampled_map_num]<0].index.tolist()\n",
    "            ub_alleles = samples_ac_df_align.loc[rxn_alleles][sampled_map_num][samples_ac_df_align.loc[rxn_alleles][sampled_map_num]>0].index.tolist()\n",
    "\n",
    "            if obj_direction==\"min\":\n",
    "                # if objective direction is min, but aligned objective coefficent (max) for \n",
    "                # rxn_min is positive, change lbs to ubs\n",
    "                \"\"\" if objective direction is min, but aligned objective coefficent (max) for \n",
    "                    rxn_min is positive, change lbs to ubs \"\"\" \n",
    "                if \"_min\" in obj_var and coef>coef_thresh:\n",
    "                    samples_ac_df_align.loc[lb_alleles,sampled_map_num] = samples_ac_df.loc[lb_alleles,sampled_map_num]*-1\n",
    "                    alleles_changed.extend(lb_alleles)\n",
    "                # if objective direction is min, but aligned objective coefficent (max) is positive, change lbs to ubs\n",
    "                if \"_max\" in obj_var and coef>coef_thresh:\n",
    "                    samples_ac_df_align.loc[ub_alleles,sampled_map_num] = samples_ac_df.loc[ub_alleles,sampled_map_num]*-1\n",
    "                    alleles_changed.extend(ub_alleles)\n",
    "                    \n",
    "            if obj_direction==\"max\":\n",
    "                if \"_min\" in obj_var and coef<-coef_thresh:\n",
    "                    samples_ac_df_align.loc[lb_alleles,sampled_map_num] = samples_ac_df.loc[lb_alleles,sampled_map_num]*-1\n",
    "                    alleles_changed.extend(lb_alleles)\n",
    "                if \"_max\" in obj_var and coef<-coef_thresh:\n",
    "                    samples_ac_df_align.loc[ub_alleles,sampled_map_num] = samples_ac_df.loc[ub_alleles,sampled_map_num]*-1\n",
    "                    alleles_changed.extend(ub_alleles)\n",
    "                    \n",
    "    return samples_ac_df_align\n",
    "\n",
    "\n",
    "def test_allele_constraint_chisquare(samples_ac_df_align, binary=True, ddof=0, double_freq=False):\n",
    "    \"\"\" Performs a chisquare test for each alleles constraints across hq models\n",
    "        if binary==True, test for whether the constraint is either upper or lower bound, not the specific value.\n",
    "    \"\"\"\n",
    "    allele_chisquare_dict = {}\n",
    "    for allele in samples_ac_df_align.index.tolist():\n",
    "        f_obs_bin = [\n",
    "            samples_ac_df_align.loc[allele][samples_ac_df_align.loc[allele]<0].shape[0],\n",
    "            samples_ac_df_align.loc[allele][samples_ac_df_align.loc[allele]>0].shape[0]\n",
    "        ]\n",
    "        if double_freq==True:\n",
    "            f_obs_bin = [x*2 for x in f_obs_bin]\n",
    "        chisq, pval_bin = stats.chisquare(f_obs_bin, f_exp=None, ddof=ddof)\n",
    "        f_obs = []\n",
    "        for bnd in list(action_constraint_mapping.values()):\n",
    "            # -2, -1, 1, 2\n",
    "            f_obs.append(samples_ac_df_align.loc[allele].values.tolist().count(bnd))\n",
    "        if double_freq==True:\n",
    "            f_obs = [x*2 for x in f_obs]\n",
    "        chisq, pval = stats.chisquare(f_obs, f_exp=None, ddof=ddof)\n",
    "        \n",
    "        # estimate entropy\n",
    "        h_obs = stats.entropy(f_obs)\n",
    "        ###       _1   _2\n",
    "        ###  LB | 0 | 1 \n",
    "        ###  UB | 4 | 0 \n",
    "        # cont_table = [[f_obs[0], f_obs[1]],[f_obs[2], f_obs[3]]]\n",
    "        # OR, pval_fisher = stats.fisher_exact(cont_table)\n",
    "        # , \"pval_fisher\":pval_fisher\n",
    "        allele_chisquare_dict.update({allele: {\"lb_ub\": f_obs_bin, \"pval_bin\": pval_bin, \"cons\": f_obs, \"pval_dis\": pval, \"h_dis\": h_obs}})\n",
    "    \n",
    "    return allele_chisquare_dict\n",
    "\n",
    "\n",
    "def get_allele_sigreacts(x, COBRA_MODEL, flux_gwas_df):\n",
    "    \"\"\" Takes in allele and FDR flux GWAS and returns the significant allele-reaction relation\"\"\"\n",
    "    # x = \"Rv0346c\"\n",
    "    allele_sig_reacts = []\n",
    "    allele_reacts = [x.id for x in COBRA_MODEL.genes.get_by_id(x).reactions]\n",
    "    for rxn in allele_reacts:\n",
    "        if rxn in flux_gwas_df.index:\n",
    "            if x in ast.literal_eval(flux_gwas_df.loc[rxn, \"genes\"]):\n",
    "                allele_sig_reacts.append(rxn)\n",
    "    return allele_sig_reacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of pyrazinamide MACs: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  1.42it/s]\n"
     ]
    }
   ],
   "source": [
    "med_or_mean='median'#\"mean\"#'median'\n",
    "bic_thresh_val = 10\n",
    "strain_numcutoff = 3\n",
    "allele_freqcutoff = 0.02\n",
    "coef_thresh = 0.0\n",
    "ddof= 0\n",
    "obj_direct_norm = True\n",
    "genename=False\n",
    "save_data=True\n",
    "\n",
    "action_constraint_mapping = ens.get_action_constraint_mapping(action_num, add_no_change=ADD_NA_BOUND)\n",
    "gene_to_name = ens.load_json_obj(\"cobra_model/gene_to_name.json\")\n",
    "\n",
    "for pheno_id in pheno_list[:]:\n",
    "    top_models = pd.read_csv(ENSEMBLE_DIR+\"/tables/best_mncs_\"+pheno_id+\".csv\",index_col=0)\n",
    "    top_models = top_models[top_models - top_models.min() < bic_thresh_val].dropna()\n",
    "    sample_list = top_models.index.tolist()\n",
    "    # sample_list = [\"sample_\"+x.split(\"_\")[-1] for x in sample_list]\n",
    "    print(\"Number of %s MACs: %d\"%(pheno_id, len(sample_list)))\n",
    "    \n",
    "    ## Get constraint dataframe\n",
    "    samples_ac_df = get_constraint_df(sample_list,pheno_to_data2d_dict,ENSEMBLE_MAP_ASSESS,gene_to_name=gene_to_name,gene_name=genename)\n",
    "    samples_ac_df.replace(action_constraint_mapping,inplace=True)\n",
    "    ## Get allele data\n",
    "    allele_list = samples_ac_df.index.tolist()\n",
    "    lor_dict = get_allele_lor_dict(allele_list,pheno_id, pheno_to_data2d_dict,pheno_to_Y_dict,gene_to_name=gene_to_name,gene_name=genename)\n",
    "    allele_data_df = pd.DataFrame.from_dict(lor_dict,orient=\"index\")\n",
    "    samples_ac_df_align = align_constraints(sample_list,samples_ac_df,ENSEMBLE_DIR,pheno_id,COBRA_MODEL, coef_thresh=coef_thresh, verbose=False)\n",
    "    \n",
    "    ## test significance of allelic constraints\n",
    "    allele_chisquare_dict = test_allele_constraint_chisquare(samples_ac_df_align, binary=True,ddof=ddof,double_freq=True)\n",
    "    allele_chisquare_df = pd.DataFrame.from_dict(allele_chisquare_dict,orient=\"index\")\n",
    "    \n",
    "    ## add significant pathway and reactions info to matrix\n",
    "    pathway_enrich_sheet = pd.read_csv(ENSEMBLE_DIR+\"/tables/pathway_enriched_FDR_\"+pheno_id+\".csv\", index_col=0)\n",
    "    flux_gwas_sheet = pd.read_csv(ENSEMBLE_DIR+\"/tables/strain_flux_med_ANOVA_df_\"+pheno_id+\"__objnorm-\"+str(obj_direct_norm)+\".csv\", index_col=0)\n",
    "    flux_gwas_sheet_correct = correct_pvals(\n",
    "        flux_gwas_sheet,pval_col=\"pvalue\",method=\"bonferroni\",correct_alpha=0.05\n",
    "    )\n",
    "    allele_chisquare_df[\"pathways\"] = allele_chisquare_df.index.map(lambda x: [x for x in gene_to_pathways[x.split(\"_\")[0]]])\n",
    "    allele_chisquare_df[\"enrich_pathways\"] = allele_chisquare_df.index.map(lambda x: [x for x in gene_to_pathways[x.split(\"_\")[0]] if x in pathway_enrich_sheet.index.tolist()])\n",
    "    # allele_chisquare_df[\"sig_reacts\"] = allele_chisquare_df.index.map(lambda x: get_allele_sigreacts(x.split(\"_\")[0], COBRA_MODEL, flux_gwas_sheet_correct))\n",
    "    allele_chisquare_df[\"sig_reacts\"] = allele_chisquare_df.index.map(lambda x: get_allele_sigreacts(x.split(\"_\")[0], COBRA_MODEL, flux_gwas_sheet_correct))\n",
    "    \n",
    "    ### Combine dataframes? Maybe wait to do this after the constraints have been organized according to min/max\n",
    "    allele_info_df = pd.concat([samples_ac_df_align, allele_data_df, allele_chisquare_df],axis=1,sort=True)\n",
    "    ### Filter out alleles that appear in < # of strains\n",
    "    allele_info_df[\"allele_freq\"] = allele_info_df[\"num_strains\"]/float(pheno_to_data2d_dict[pheno_id].shape[0])\n",
    "    allele_info_df = allele_info_df[allele_info_df[\"num_strains\"]>strain_numcutoff]\n",
    "    allele_info_df = allele_info_df[allele_info_df[\"allele_freq\"]>allele_freqcutoff]\n",
    "    allele_info_df[\"allele_rv\"] = allele_info_df.index\n",
    "    ### Get names\n",
    "    allele_info_df = func_convert_gene2name_df(allele_info_df, gene_to_name)\n",
    "    allele_info_df[\"gene\"] = allele_info_df.index.map(lambda x: x.split(\"_\")[0])\n",
    "    allele_info_df.sort_values([\"pval_bin\", \"lor\"], ascending=[True,False],inplace=True)\n",
    "    allele_info_df[\"med_bnd\"] = allele_info_df[sample_list].median(axis=1)\n",
    "    if save_data==True:\n",
    "        allele_info_df.to_csv(ENSEMBLE_DIR+\"/tables/allelic_effects_\"+pheno_id+\"_ddof-\"+str(ddof)+\"_cutoff-\"+str(np.round(coef_thresh, 4))+\".csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate supplement after running cells above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_objective_sheet(ENSEMBLE_DIR, pheno_id):\n",
    "    obj_hqsamples_abs_df = pd.read_csv(ENSEMBLE_DIR+\"/tables/mnc_objectives_\"+pheno_id+\"__MAX-ABS\"+\".csv\",index_col=0)\n",
    "    obj_med_abs_df = pd.read_csv(ENSEMBLE_DIR+\"/tables/mnc_objectives_\"+pheno_id+\"__med-abs\"+\".csv\",index_col=0,header=None,squeeze=True)\n",
    "    obj_med_abs_df.index.name = None\n",
    "    obj_med_abs_df.name = \"med-abs\"\n",
    "    obj_hqsamples_abs_df = obj_hqsamples_abs_df.T\n",
    "    obj_df = pd.concat([obj_med_abs_df, obj_hqsamples_abs_df],axis=1)\n",
    "    obj_df.sort_values([\"med-abs\"],ascending=False, inplace=True)\n",
    "    return obj_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/erolkavvas/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:7 \u001b[1;31mFutureWarning\u001b[0m: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "/Users/erolkavvas/anaconda3/lib/python3.7/site-packages/statsmodels/stats/multitest.py:147 \u001b[1;31mRuntimeWarning\u001b[0m: invalid value encountered in less_equal\n",
      "/Users/erolkavvas/anaconda3/lib/python3.7/site-packages/statsmodels/stats/multitest.py:251 \u001b[1;31mRuntimeWarning\u001b[0m: invalid value encountered in greater\n"
     ]
    }
   ],
   "source": [
    "# bic_thresh_val = 10\n",
    "coef_thresh = 0.0\n",
    "obj_direct_norm = True\n",
    "\n",
    "for pheno_id in pheno_list:\n",
    "    \n",
    "    ### characters are too long for excel sheet so shorten to PAS\n",
    "    if len(pheno_id) >= 21:\n",
    "        pheno_id_name = pheno_id[:10]\n",
    "    else:\n",
    "        pheno_id_name = pheno_id\n",
    "            \n",
    "    ### Sheet 1 - outline of other sheets\n",
    "    outline_df = pd.DataFrame.from_dict({\n",
    "        pheno_id_name+\"_MAC_modelBIC\": 'List of best MACs with delta BIC < 10',\n",
    "        pheno_id_name+\"_MAC_objectives\": 'Objective functions for best MACs (absolute). First column is median absolute coefficient.',\n",
    "        pheno_id_name+\"_MAC_flux_GWAS\": 'Univariate statistical tests between median strain-specific MAC reaction fluxes (minmax scaled) and AMR with Bonferroni-corrted p-value<0.05',\n",
    "        pheno_id_name+\"_MAC_shprice_GWAS\": 'Univariate statistical tests between median strain-specific MAC metabolite shadow prices (minmax scaled) and AMR with Bonferroni-corrected p-value<0.05',\n",
    "        pheno_id_name+\"_MAC_pathway_enrich\": 'Pathway enrichments of significant fluxes with FDR<0.05',\n",
    "        pheno_id_name+\"_MAC_allele_params\": 'Allele-specific information in best MACs such as constraints, mutations, etc. Ranked according to chi-squared goodness of fit. Constraints were aligned in direction of objective reaction variable coefficient.',\n",
    "    },orient=\"index\")\n",
    "    outline_df.columns=[\"sheet description for \"+pheno_id]\n",
    "\n",
    "    with pd.ExcelWriter(SUPP_FILE_LOC+'Supplementary file '+pheno_id+'.xlsx') as writer:\n",
    "        \n",
    "        outline_df.to_excel(writer, sheet_name=pheno_id_name+\" supp outline\")\n",
    "        \n",
    "        top_models = pd.read_csv(ENSEMBLE_DIR+\"/tables/best_mncs_\"+pheno_id+\".csv\",index_col=0)\n",
    "        top_models = top_models[top_models - top_models.min() < bic_thresh_val].dropna()\n",
    "        top_models.to_excel(writer, sheet_name=pheno_id_name+\"_MAC_modelBIC\")\n",
    "        \n",
    "        objective_sheet = get_objective_sheet(ENSEMBLE_DIR, pheno_id)\n",
    "        objective_sheet.to_excel(writer, sheet_name=pheno_id_name+\"_MAC_objectives\")\n",
    "        \n",
    "        flux_gwas_sheet = pd.read_csv(ENSEMBLE_DIR+\"/tables/strain_flux_med_ANOVA_df_\"+pheno_id+\"__objnorm-\"+str(obj_direct_norm)+\".csv\", index_col=0)\n",
    "        flux_gwas_sheet_correct = correct_pvals(\n",
    "            flux_gwas_sheet,pval_col=\"pvalue\",method=\"bonferroni\",correct_alpha=0.05\n",
    "        )\n",
    "        flux_gwas_sheet_correct.to_excel(writer, sheet_name=pheno_id_name+\"_MAC_flux_GWAS\")\n",
    "        if SPRICE_RCOST_BOOL==True:\n",
    "            sprice_gwas_sheet = pd.read_csv(ENSEMBLE_DIR+\"/tables/strain_sprice_med_ANOVA_df_\"+pheno_id+\"__objnorm-\"+str(obj_direct_norm)+\".csv\",index_col=0)\n",
    "            sprice_gwas_sheet_correct = correct_pvals(\n",
    "                sprice_gwas_sheet,pval_col=\"pvalue\",method=\"bonferroni\",correct_alpha=0.05\n",
    "            )\n",
    "            sprice_gwas_sheet_correct.to_excel(writer, sheet_name=pheno_id_name+\"_MAC_shprice_GWAS\")\n",
    "        \n",
    "        pathway_enrich_sheet = pd.read_csv(ENSEMBLE_DIR+\"/tables/pathway_enriched_FDR_\"+pheno_id+\".csv\", index_col=0)\n",
    "        pathway_enrich_sheet.to_excel(writer, sheet_name=pheno_id_name+\"_MAC_pathway_enrich\")\n",
    "        \n",
    "        allele_info_df = pd.read_csv(ENSEMBLE_DIR+\"/tables/allelic_effects_\"+pheno_id+\"_ddof-\"+str(ddof)+\"_cutoff-\"+str(np.round(coef_thresh, 4))+\".csv\",index_col=0)\n",
    "        allele_info_df.to_excel(writer, sheet_name=pheno_id_name+\"_MAC_allele_params\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
